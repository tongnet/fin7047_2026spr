{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96fef500",
      "metadata": {
        "id": "96fef500",
        "outputId": "8d3e97bf-2d2c-4c63-f1be-e8d363a0e1dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\yut3\\appdata\\local\\anaconda3\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in c:\\users\\yut3\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\yut3\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yut3\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in c:\\users\\yut3\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\yut3\\appdata\\local\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\yut3\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\yut3\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\yut3\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\yut3\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "Hello there! Welcome to Finance 7047. This course, which is offered in Spring 2025, \n",
            "is designed to provide a good understanding of Financial Technologies and Cryptocurrency.\n",
            "\n",
            "After Lowercasing:\n",
            "hello there! welcome to finance 7047. this course, which is offered in spring 2025, \n",
            "is designed to provide a good understanding of financial technologies and cryptocurrency.\n",
            "\n",
            "After Removing Punctuation:\n",
            "hello there welcome to finance 7047 this course which is offered in spring 2025 \n",
            "is designed to provide a good understanding of financial technologies and cryptocurrency\n",
            "\n",
            "After Tokenization:\n",
            "['hello', 'there', 'welcome', 'to', 'finance', '7047', 'this', 'course', 'which', 'is', 'offered', 'in', 'spring', '2025', 'is', 'designed', 'to', 'provide', 'a', 'good', 'understanding', 'of', 'financial', 'technologies', 'and', 'cryptocurrency']\n",
            "\n",
            "After Removing Stopwords:\n",
            "['hello', 'welcome', 'finance', '7047', 'course', 'offered', 'spring', '2025', 'designed', 'provide', 'good', 'understanding', 'financial', 'technologies', 'cryptocurrency']\n",
            "\n",
            "After Stemming:\n",
            "['hello', 'welcom', 'financ', '7047', 'cours', 'offer', 'spring', '2025', 'design', 'provid', 'good', 'understand', 'financi', 'technolog', 'cryptocurr']\n",
            "\n",
            "After Lemmatization:\n",
            "['hello', 'welcome', 'finance', '7047', 'course', 'offered', 'spring', '2025', 'designed', 'provide', 'good', 'understanding', 'financial', 'technology', 'cryptocurrency']\n",
            "\n",
            "Final Preprocessed Text:\n",
            "hello welcome finance 7047 course offered spring 2025 designed provide good understanding financial technology cryptocurrency\n"
          ]
        }
      ],
      "source": [
        "# NLPK procedure\n",
        "\n",
        "!pip install nltk\n",
        "\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"\"\"Hello there! Welcome to Finance 7047. This course, which is offered in Spring 2025,\n",
        "is designed to provide a good understanding of Financial Technologies and Cryptocurrency.\"\"\"\n",
        "\n",
        "print(f\"Original Text:\\n{sample_text}\\n\")\n",
        "\n",
        "# Step 1: Convert to Lowercase\n",
        "lowercase_text = sample_text.lower()\n",
        "print(f\"After Lowercasing:\\n{lowercase_text}\\n\")\n",
        "\n",
        "# Step 2: Remove Punctuation\n",
        "# Using regular expressions to remove punctuation marks\n",
        "cleaned_text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", lowercase_text)\n",
        "print(f\"After Removing Punctuation:\\n{cleaned_text}\\n\")\n",
        "\n",
        "# Step 3: Tokenization\n",
        "# Split text into words (tokens)\n",
        "tokens = word_tokenize(cleaned_text)\n",
        "print(f\"After Tokenization:\\n{tokens}\\n\")\n",
        "\n",
        "# Step 4: Remove Stopwords\n",
        "# Load NLTK's list of English stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(f\"After Removing Stopwords:\\n{filtered_tokens}\\n\")\n",
        "\n",
        "# Step 5: Stemming (Optional)\n",
        "# Use Porter Stemmer to reduce words to their root form\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(f\"After Stemming:\\n{stemmed_tokens}\\n\")\n",
        "\n",
        "# Step 6: Lemmatization (Alternative to Stemming)\n",
        "# Use WordNet Lemmatizer to get the base form of words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "print(f\"After Lemmatization:\\n{lemmatized_tokens}\\n\")\n",
        "\n",
        "# Final Preprocessed Text\n",
        "final_text = \" \".join(lemmatized_tokens)\n",
        "print(f\"Final Preprocessed Text:\\n{final_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032d9f64",
      "metadata": {
        "id": "032d9f64",
        "outputId": "f7c1c276-a2d2-4af3-a31e-64cdb4d6419e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'textblob'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TextBlob -- sentiment analysis\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#!pip install textblob\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Sample sentences\u001b[39;00m\n\u001b[0;32m      7\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love this product! It’s absolutely fantastic.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the worst experience I have ever had.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am extremely disappointed with the quality of this item.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m ]\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
          ]
        }
      ],
      "source": [
        "# TextBlob -- sentiment analysis\n",
        "\n",
        "#!pip install textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"I love this product! It’s absolutely fantastic.\",\n",
        "    \"This is the worst experience I have ever had.\",\n",
        "    \"I'm not very happy with the service.\",\n",
        "    \"The movie was great and very enjoyable!\",\n",
        "    \"I am extremely disappointed with the quality of this item.\"\n",
        "]\n",
        "\n",
        "# Perform sentiment analysis on each sentence\n",
        "for sentence in sentences:\n",
        "    blob = TextBlob(sentence)  # Create a TextBlob object\n",
        "    sentiment_score = blob.sentiment.polarity  # Get the polarity score (-1 to 1)\n",
        "\n",
        "    # Determine sentiment based on polarity score\n",
        "    if sentiment_score > 0:\n",
        "        sentiment = \"Positive\"\n",
        "    elif sentiment_score < 0:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "\n",
        "    # Print the sentence and its corresponding sentiment\n",
        "    print(f\"Sentence: {sentence}\\nSentiment: {sentiment}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc84d365",
      "metadata": {
        "id": "fc84d365"
      },
      "outputs": [],
      "source": [
        "# Topic modeling\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import pandas as pd\n",
        "\n",
        "# Sample documents for topic modeling\n",
        "documents = [\n",
        "    \"I love to eat pizza and pasta.\",\n",
        "    \"The economy is facing a downturn with rising unemployment.\",\n",
        "    \"Pizza and burgers are my favorite fast foods.\",\n",
        "    \"The government announced new policies to tackle inflation.\",\n",
        "    \"AI and machine learning are transforming the technology industry.\",\n",
        "    \"The stock market is experiencing a significant drop.\",\n",
        "    \"Artificial intelligence is changing how we interact with technology.\",\n",
        "    \"The economic impact of the pandemic has been severe.\"\n",
        "]\n",
        "# Step 1: Vectorize the text (Bag-of-Words)\n",
        "# Use CountVectorizer to create the document-term matrix\n",
        "vectorizer = CountVectorizer(stop_words='english')  # Remove common English stopwords\n",
        "X = vectorizer.fit_transform(documents)\n",
        "# Step 2: Apply LDA to extract topics\n",
        "lda = LatentDirichletAllocation(n_components=2, random_state=42)  # Number of topics set to 2\n",
        "lda.fit(X)\n",
        "\n",
        "# Step 3: Display the top words in each topic\n",
        "num_top_words = 5  # Number of top words to display for each topic\n",
        "feature_names = vectorizer.get_feature_names_out()  # Get the feature names (words)\n",
        "\n",
        "# Create a dictionary to store top words for each topic\n",
        "topics = {}\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    topics[f'Topic #{topic_idx + 1}'] = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
        "\n",
        "# Convert to a DataFrame for easy display\n",
        "topics_df = pd.DataFrame(topics)\n",
        "\n",
        "# Display the DataFrame with the top words for each topic\n",
        "print(\"Top Words in Each Topic:\")\n",
        "print(topics_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "895db905",
      "metadata": {
        "id": "895db905"
      },
      "outputs": [],
      "source": [
        "# BoW\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define a sample set of documents\n",
        "documents = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog barked loudly.\",\n",
        "    \"The cat chased the mouse.\",\n",
        "]\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the model and transform the documents into a BoW matrix\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the result into a dense matrix format and print it\n",
        "print(\"BoW Matrix (Document-Term Matrix):\\n\", bow_matrix.toarray())\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "print(\"Feature Names (Vocabulary):\\n\", vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4402a094",
      "metadata": {
        "id": "4402a094"
      },
      "outputs": [],
      "source": [
        "# Read in a large file\n",
        "\n",
        "import PyPDF2\n",
        "\n",
        "# Define the PDF file path\n",
        "file_path = \"nakamoto_2008_bitcoin.pdf\"\n",
        "\n",
        "# Open the PDF file and extract text\n",
        "with open(file_path, \"rb\") as file:\n",
        "    reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "    # Extract text from all pages\n",
        "    large_document = \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
        "\n",
        "# Print the first 500 characters as a preview\n",
        "print(large_document[:500])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0027cd",
      "metadata": {
        "id": "2e0027cd"
      },
      "outputs": [],
      "source": [
        "# Embeding\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample documents\n",
        "documents = [\"The cat ran.\", \"The dog barked.\"]\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Tokenize sentences for Word2Vec\n",
        "tokenized_sentences = [word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=3, min_count=1)\n",
        "\n",
        "# Display vocabulary\n",
        "print(\"Vocabulary:\", list(model.wv.index_to_key))\n",
        "\n",
        "# Find words similar to \"dog\"\n",
        "print(\"Most similar to 'dog':\", model.wv.most_similar(\"dog\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a674ef1",
      "metadata": {
        "id": "7a674ef1",
        "outputId": "3e59744f-0b2c-4b53-a7de-20a48f90b90f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the sentences provided, the words 'dog' and 'cat' play central roles, and their relationships with other words in the context indicate a number of similarities and differences concerning their functions and attributes:\n",
            "\n",
            "1. **Sentence Analysis**:\n",
            "   - **Sentence 1**: \"The quick brown fox jumps over the lazy dog.\"\n",
            "     - 'Dog' is described as 'lazy' and is part of a passive scene where the action (jumping) is performed by another animal (fox).\n",
            "   - **Sentence 2**: \"The dog chased the cat.\"\n",
            "     - Here, 'dog' is active, performing the action of chasing. 'Cat' is the object of the chase, implying a dynamic interaction where the dog is the aggressor and the cat potentially the victim or a participant in a playful or antagonistic scenario.\n",
            "   - **Sentence 3**: \"The cat climbed the tree.\"\n",
            "     - 'Cat' is the subject performing the action (climbing), indicating agility and possibly a reaction (maybe even to being chased as could be inferred from the previous sentence).\n",
            "   - **Sentence 4**: \"The dog barked loudly at the intruder.\"\n",
            "     - 'Dog' is again in an active role, this time vocalizing aggressively or defensively towards an intruder, showcasing a protective or alert behavior.\n",
            "\n",
            "2. **Relationship Analysis**:\n",
            "   - **Roles**: Both 'dog' and 'cat' are portrayed as capable of dynamic and active roles. The dog engages in chasing and barking, activities that imply energy and possibly protectiveness. The cat is shown climbing, a skillful activity requiring agility.\n",
            "   - **Actions and Interactions**: In Sentence 2, the direct interaction between 'dog' and 'cat' involves chasing, a common depiction of their relationship playing on the stereotypical notion of dogs chasing cats. This shows them in a potentially adversarial or playful context, depending on interpretation.\n",
            "   - **Attributes**: The dog is described in various roles that reflect common canine attributes such as being lazy, protective, and possibly loud. The cat is depicted performing an action that reflects typical feline attributes like agility and climbing ability.\n",
            "\n",
            "3. **Semantic Similarities**:\n",
            "   - Both animals are portrayed in roles that reflect agility and activity. Even the 'lazy' descriptor for the dog is a common antithetical trait used to highlight moments of inactivity, which contrasts with its other active portrayals.\n",
            "   - They are central figures in their respective sentences, pointing to their importance as subjects and objects in narratives about animal behavior.\n",
            "\n",
            "In summary, 'dog' and 'cat' in these contexts are used to demonstrate typical behaviors and attributes associated with each, often showcasing their physical abilities and interactions. The dynamic between the two in these sentences plays on traditional notions of their interaction (chasing), while also highlighting unique characteristics (climbing, barking) that define their respective species in popular narratives.\n"
          ]
        }
      ],
      "source": [
        "# Textual analysis using GPT\n",
        "\n",
        "import openai\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = \"\"  # Replace with your actual API key\n",
        "\n",
        "\n",
        "# Define messages for chat-based models\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "    Given the sentences below, analyze the relationships between words and describe the similarities:\n",
        "    1. \"The quick brown fox jumps over the lazy dog.\"\n",
        "    2. \"The dog chased the cat.\"\n",
        "    3. \"The cat climbed the tree.\"\n",
        "    4. \"The dog barked loudly at the intruder.\"\n",
        "    How are the words 'dog' and 'cat' related in these contexts?\n",
        "    \"\"\"}\n",
        "]\n",
        "\n",
        "# Make the API call to OpenAI\n",
        "client = openai.OpenAI()  # New API client format\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo\",  # Use the latest available model\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "125da8dc",
      "metadata": {
        "id": "125da8dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa9699e7",
      "metadata": {
        "id": "fa9699e7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}